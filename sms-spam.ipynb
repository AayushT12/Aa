{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "30d6db93",
      "metadata": {
        "id": "30d6db93"
      },
      "outputs": [],
      "source": [
        "#importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import string\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e527796",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "1e527796",
        "outputId": "0d0d6a53-8f53-41d2-ee32-07df8be56955"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sms-spam.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dfb9ea6f13bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reading the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sms-spam.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#displaying the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sms-spam.csv'"
          ]
        }
      ],
      "source": [
        "#reading the CSV file\n",
        "x = pd.read_csv(\"sms-spam.csv\")\n",
        "\n",
        "#displaying the dataframe\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78693fdd",
      "metadata": {
        "id": "78693fdd"
      },
      "outputs": [],
      "source": [
        "#dataset size - 5572 rows x 5 columns\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6719610c",
      "metadata": {
        "id": "6719610c"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226dc4bb",
      "metadata": {
        "id": "226dc4bb"
      },
      "outputs": [],
      "source": [
        "#printing the concise summary of the dataset\n",
        "x.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1205abf7",
      "metadata": {
        "id": "1205abf7"
      },
      "outputs": [],
      "source": [
        "#column 2, 3, 4 have majority missing values, so it is better to drop them.\n",
        "x.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace = True)\n",
        "\n",
        "#displaying the edited dataframe\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5f0034",
      "metadata": {
        "id": "7a5f0034"
      },
      "outputs": [],
      "source": [
        "#renaming the column names to a better and meaningful column name\n",
        "x.rename(columns = {'v1':'result', 'v2':'input'}, inplace=True)\n",
        "\n",
        "#displaying the edited dataframe\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd46be7",
      "metadata": {
        "id": "3cd46be7"
      },
      "outputs": [],
      "source": [
        "#result has categorical labels, we need to convert it into numerical values - enbcoding\n",
        "#for that we will be using 'LabelEncoder' from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a001e3b",
      "metadata": {
        "id": "6a001e3b"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "x['result'] = encoder.fit_transform(x['result'])\n",
        "\n",
        "#displaying the edited dataframe\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215b7f36",
      "metadata": {
        "id": "215b7f36"
      },
      "outputs": [],
      "source": [
        "#so 0 means no SPAM, 1 means SPAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa8ba1ab",
      "metadata": {
        "id": "fa8ba1ab"
      },
      "outputs": [],
      "source": [
        "#check if there is any NULL value\n",
        "x.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95877ee",
      "metadata": {
        "id": "c95877ee"
      },
      "outputs": [],
      "source": [
        "#the dataset has NO null values, so don't need to handle them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aad25e1",
      "metadata": {
        "id": "7aad25e1"
      },
      "outputs": [],
      "source": [
        "#check if there is any DUPLICATE values\n",
        "x.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a5defd",
      "metadata": {
        "id": "c1a5defd"
      },
      "outputs": [],
      "source": [
        "#the dataset has DUPLICATE values, so we will have to REMOVE them\n",
        "x = x.drop_duplicates(keep='first')\n",
        "\n",
        "#displaying the edited dataframe\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a120833",
      "metadata": {
        "id": "1a120833"
      },
      "outputs": [],
      "source": [
        "#rows reduced from 5572 to 5169 after DUPLICATED values have been deleted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9d382c",
      "metadata": {
        "id": "6d9d382c"
      },
      "source": [
        "## EDA - Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf35d200",
      "metadata": {
        "id": "bf35d200"
      },
      "outputs": [],
      "source": [
        "#the given problem is a classification problem, so we need to understand the data first by performing EDA.\n",
        "#the dataset has only 2 columns, so less analysis required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1758d20",
      "metadata": {
        "id": "f1758d20"
      },
      "outputs": [],
      "source": [
        "#checking the number of SPAM vs not SPAM messages\n",
        "x['result'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0e68d4",
      "metadata": {
        "id": "8f0e68d4"
      },
      "outputs": [],
      "source": [
        "#out of 5169 datavalues, 653 are SPAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06cd5547",
      "metadata": {
        "id": "06cd5547"
      },
      "outputs": [],
      "source": [
        "653*100.0/5169"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde6542b",
      "metadata": {
        "id": "fde6542b"
      },
      "outputs": [],
      "source": [
        "#12.63% is SPAM and 87.37% is not SPAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0827b7",
      "metadata": {
        "id": "ea0827b7"
      },
      "outputs": [],
      "source": [
        "#for better representation, we can use PIE CHART to represent it.\n",
        "#PIE CHARTS can be created using matplotlib library\n",
        "\n",
        "plt.pie(x['result'].value_counts(),  labels = ['NOT SPAM', 'SPAM'], autopct = '%0.2f', radius = 0.8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07846e7",
      "metadata": {
        "id": "d07846e7"
      },
      "outputs": [],
      "source": [
        "#hence, highly IMBALANCED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d774e6d0",
      "metadata": {
        "id": "d774e6d0"
      },
      "outputs": [],
      "source": [
        "#now we will be analysing the number of alphabets/words/sentences being used in the TEXT\n",
        "#for this, will create 3 new columns: (1) no. of characters (2) no. of words (3) no. of sentences in SMS\n",
        "\n",
        "#using 'nltk' library for this.\n",
        "#Natural Language Toolkit for text processing\n",
        "#(pip install nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd4ce2b",
      "metadata": {
        "id": "ffd4ce2b"
      },
      "outputs": [],
      "source": [
        "#downloading the dependencies\n",
        "#punkt package includes pre-trained models for tokenizing text in many languages\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b0ae5a",
      "metadata": {
        "id": "e0b0ae5a"
      },
      "outputs": [],
      "source": [
        "#creating a new column with count of characters\n",
        "x['countCharacters'] = x['input'].apply(len)\n",
        "\n",
        "#creating a new column with count of words\n",
        "x['countWords'] = x['input'].apply(lambda i:len(nltk.word_tokenize(i)))\n",
        "#'word_tokenize' function takes a string of text as input and returns a list of words\n",
        "\n",
        "#creating a new column with count of sentences\n",
        "x['countSentences'] = x['input'].apply(lambda i:len(nltk.sent_tokenize(i)))\n",
        "#'sent_tokenize' function takes a string of text as input and returns a list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2deabf7",
      "metadata": {
        "id": "b2deabf7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#displaying the edited dataframe with the 3 new columns added\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a255a1",
      "metadata": {
        "id": "e9a255a1"
      },
      "outputs": [],
      "source": [
        "#extracting the 5 number summary of the 3 new column values\n",
        "x[['countCharacters', 'countWords', 'countSentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd14494",
      "metadata": {
        "id": "8bd14494"
      },
      "outputs": [],
      "source": [
        "#extracting the same summaries, classified on the basis of SPAM and not SPAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1db23a",
      "metadata": {
        "id": "8a1db23a"
      },
      "outputs": [],
      "source": [
        "#for not SPAM\n",
        "x[x['result'] == 0][['countCharacters', 'countWords', 'countSentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415a82ca",
      "metadata": {
        "id": "415a82ca",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#for SPAM\n",
        "x[x['result'] == 1][['countCharacters', 'countWords', 'countSentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbff4800",
      "metadata": {
        "id": "cbff4800"
      },
      "outputs": [],
      "source": [
        "#for better visualization, we will plot a histogram using 'seaborn'\n",
        "plt.figure(figsize = (15, 5))\n",
        "sns.histplot(x[x['result'] == 0]['countCharacters'], color = \"yellow\")\n",
        "sns.histplot(x[x['result'] == 1]['countCharacters'], color = \"black\")\n",
        "\n",
        "#black -> SPAM, yellow -> not SPAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4ac0dad",
      "metadata": {
        "id": "d4ac0dad"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15, 5))\n",
        "sns.histplot(x[x['result'] == 0]['countWords'], color = \"yellow\")\n",
        "sns.histplot(x[x['result'] == 1]['countWords'], color = \"black\")\n",
        "\n",
        "#black -> SPAM, yellow -> not SPAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "158938fd",
      "metadata": {
        "id": "158938fd"
      },
      "outputs": [],
      "source": [
        "#observation : SPAM messages have more no of characters used, mean is 137.89 for SPAM and 70.45 for not SPAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4bb03f",
      "metadata": {
        "id": "2b4bb03f"
      },
      "outputs": [],
      "source": [
        "#finding relationship between the columns\n",
        "sns.pairplot(x, hue='result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ade1fe",
      "metadata": {
        "id": "d9ade1fe"
      },
      "outputs": [],
      "source": [
        "#find pearson's correlation coefficient\n",
        "x.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc6551d",
      "metadata": {
        "id": "bfc6551d"
      },
      "outputs": [],
      "source": [
        "#converting it into a heatmap\n",
        "sns.heatmap(x.corr(), annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ca076e",
      "metadata": {
        "id": "28ca076e"
      },
      "outputs": [],
      "source": [
        "#multi-collinearity in the dataset\n",
        "#all new 3 columns are highly correlated with each other but countCharacters is correlated more with the 'result' than any other column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b916c85d",
      "metadata": {
        "id": "b916c85d"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78770422",
      "metadata": {
        "id": "78770422"
      },
      "outputs": [],
      "source": [
        "#peforming preprocessing such as tokenization (converting the text into tokens or words), removing special characters,\n",
        "#removing stop words and punctuation and finallying stemming the data.\n",
        "#also, converting to lower case first and then pre-processing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727bd8e8",
      "metadata": {
        "id": "727bd8e8"
      },
      "outputs": [],
      "source": [
        "#downloading the package which contains the stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb37349b",
      "metadata": {
        "id": "fb37349b"
      },
      "outputs": [],
      "source": [
        "def transform_text (text):\n",
        "\n",
        "    #converting to lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    #tokenization\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    #removing special characters\n",
        "    removedSC = list()\n",
        "    for i in text:\n",
        "        if i.isalnum():\n",
        "            removedSC.append(i)\n",
        "\n",
        "    #updating the text after removed special characters\n",
        "    text = removedSC[:]\n",
        "\n",
        "    #removing stop words and punctuation characters\n",
        "    removedSWPC = list()\n",
        "    for i in text:\n",
        "        #stopwords.words('english') is a function of 'nltk', returns list of english stop words\n",
        "        #string.punctuation is a part of 'string' module, containing the ASCII punctuation characters\n",
        "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
        "            removedSWPC.append(i)\n",
        "\n",
        "    #updating the text after removed stop words and punctuation characters\n",
        "    text = removedSWPC[:]\n",
        "\n",
        "    #stemming the data using 'PorterStemmer' algorithm.\n",
        "    #nltk module provides this class to use.\n",
        "    ps = PorterStemmer()\n",
        "    stemmed = list()\n",
        "    for i in text:\n",
        "        stemmed.append(ps.stem(i))\n",
        "\n",
        "    text = stemmed[:]\n",
        "\n",
        "    return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6589a86c",
      "metadata": {
        "id": "6589a86c"
      },
      "outputs": [],
      "source": [
        "#function for transforming the text is ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2096abcc",
      "metadata": {
        "id": "2096abcc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#will create a new column to store the transformed text -> 'processed'\n",
        "x['processed'] = x['input'].apply(transform_text)\n",
        "\n",
        "#displaying the edited dataframe with a new column 'processed'\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52148949",
      "metadata": {
        "id": "52148949"
      },
      "outputs": [],
      "source": [
        "#will be creating word cloud for data visualization to display the most frequently occurring words in the processed dataset.\n",
        "#using 'WordCloud' class\n",
        "\n",
        "wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "475f1006",
      "metadata": {
        "id": "475f1006"
      },
      "outputs": [],
      "source": [
        "#creating a wordcloud for the SPAM messages\n",
        "spamWC = wc.generate(x[x['result'] == 1]['processed'].str.cat(sep=\" \"))\n",
        "\n",
        "#creating figure and displaying\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(spamWC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KKjzvZxw9fA7",
      "metadata": {
        "id": "KKjzvZxw9fA7"
      },
      "outputs": [],
      "source": [
        "#creating a wordcloud for the not SPAM messages\n",
        "spamWC = wc.generate(x[x['result'] == 0]['processed'].str.cat(sep=\" \"))\n",
        "\n",
        "#creating figure and displaying\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(spamWC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kjNkfXx793fG",
      "metadata": {
        "id": "kjNkfXx793fG"
      },
      "outputs": [],
      "source": [
        "#extracting the most common words used in both SPAM and not SPAM messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0GM55U9i-rfk",
      "metadata": {
        "id": "0GM55U9i-rfk"
      },
      "outputs": [],
      "source": [
        "#extracting all the words used in SPAM messages\n",
        "spamWords = list()\n",
        "\n",
        "for msg in x[x['result'] == 1]['processed'].tolist():\n",
        "  for word in msg.split():\n",
        "    spamWords.append(word)\n",
        "\n",
        "spamWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Us_mXRt5-ySp",
      "metadata": {
        "id": "Us_mXRt5-ySp"
      },
      "outputs": [],
      "source": [
        "#to count the frequency of the words, we will be using the Counter class to create a dictionary\n",
        "spamWordsDictionary = Counter(spamWords)\n",
        "\n",
        "#to extract the most common words\n",
        "spamWordsDictionary.most_common(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fDKbm3uH_Nph",
      "metadata": {
        "id": "fDKbm3uH_Nph"
      },
      "outputs": [],
      "source": [
        "#converting this dictionary to a dataframe\n",
        "mostCommonSPAM = pd.DataFrame(spamWordsDictionary.most_common(40))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jBblxZIk_30g",
      "metadata": {
        "id": "jBblxZIk_30g"
      },
      "outputs": [],
      "source": [
        "#plotting a bar plot of the mostCommonSPAM dataframe\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data = mostCommonSPAM, x=0, y=1)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iHYyTWYDAO2p",
      "metadata": {
        "id": "iHYyTWYDAO2p"
      },
      "outputs": [],
      "source": [
        "#words like 'CALL', 'FREE', '2', 'TXT', 'TEXT', 'UR', 'MOBIL' are the most common words in SPAM texts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hD1ryYyPDoUS",
      "metadata": {
        "id": "hD1ryYyPDoUS"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g4CYS7lVD02K",
      "metadata": {
        "id": "g4CYS7lVD02K"
      },
      "outputs": [],
      "source": [
        "#NaiveBayes classifier works BEST on textual data, so will firstly perform it on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-WxWmcFCEAFI",
      "metadata": {
        "id": "-WxWmcFCEAFI"
      },
      "outputs": [],
      "source": [
        "#we need to give numerical inputs to the classifier model, so will have to convert the 'processed' column into vectors.\n",
        "#using 'bag of words'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GXzUN--UEznA",
      "metadata": {
        "id": "GXzUN--UEznA"
      },
      "outputs": [],
      "source": [
        "#converting the collection of text into a matrix of token counts\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zdPlxYWXE-vm",
      "metadata": {
        "id": "zdPlxYWXE-vm"
      },
      "outputs": [],
      "source": [
        "#transforming the data of processed column\n",
        "X = cv.fit_transform(x['processed']).toarray()\n",
        "\n",
        "#printing size of X\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UaAee-QhFDdf",
      "metadata": {
        "id": "UaAee-QhFDdf"
      },
      "outputs": [],
      "source": [
        "#storing the values of the 'result' column\n",
        "y = x['result'].values\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2LZrgCI0FY8y",
      "metadata": {
        "id": "2LZrgCI0FY8y"
      },
      "outputs": [],
      "source": [
        "#splitting the training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 49)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_3FTO2RHQuv",
      "metadata": {
        "id": "0_3FTO2RHQuv"
      },
      "outputs": [],
      "source": [
        "#creating the objects for the models\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "bnb = BernoulliNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A3-_9K4ZH3nO",
      "metadata": {
        "id": "A3-_9K4ZH3nO"
      },
      "outputs": [],
      "source": [
        "#training the dataset for GaussianNB\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred1 = gnb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred1))\n",
        "print(confusion_matrix(y_test, y_pred1))\n",
        "print(precision_score(y_test, y_pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nt9Ox6u6IDeC",
      "metadata": {
        "id": "nt9Ox6u6IDeC"
      },
      "outputs": [],
      "source": [
        "#training the dataset for MultinomialnNB\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred2 = mnb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred2))\n",
        "print(confusion_matrix(y_test, y_pred2))\n",
        "print(precision_score(y_test, y_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Qiq7GWJJSLo",
      "metadata": {
        "id": "-Qiq7GWJJSLo"
      },
      "outputs": [],
      "source": [
        "#training the dataset for BernoulliNB\n",
        "bnb.fit(X_train, y_train)\n",
        "y_pred3 = bnb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred3))\n",
        "print(confusion_matrix(y_test, y_pred3))\n",
        "print(precision_score(y_test, y_pred3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BVi02ol7JYYb",
      "metadata": {
        "id": "BVi02ol7JYYb"
      },
      "outputs": [],
      "source": [
        "#we have to focus mainly on 'precision' value\n",
        "#the max precision we got is 9.45 with 9.64 as accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m3_35xosKV9Y",
      "metadata": {
        "id": "m3_35xosKV9Y"
      },
      "outputs": [],
      "source": [
        "#using 'TfidfVectorizer' for vectorization\n",
        "tf = TfidfVectorizer()\n",
        "\n",
        "#transforming the data of processed column\n",
        "X = tf.fit_transform(x['processed']).toarray()\n",
        "\n",
        "#storing the values of the 'result' column\n",
        "y = x['result'].values\n",
        "\n",
        "#splitting the training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 49)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2S1dzhqsKjG0",
      "metadata": {
        "id": "2S1dzhqsKjG0"
      },
      "outputs": [],
      "source": [
        "#training the dataset for GaussianNB\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred1 = gnb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred1))\n",
        "print(confusion_matrix(y_test, y_pred1))\n",
        "print(precision_score(y_test, y_pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9DsDpIpNK0PX",
      "metadata": {
        "id": "9DsDpIpNK0PX"
      },
      "outputs": [],
      "source": [
        "#training the dataset for MultinomialnNB\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred2 = mnb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred2))\n",
        "print(confusion_matrix(y_test, y_pred2))\n",
        "print(precision_score(y_test, y_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0qkJKFHZK065",
      "metadata": {
        "id": "0qkJKFHZK065"
      },
      "outputs": [],
      "source": [
        "#training the dataset for BernoulliNB\n",
        "bnb.fit(X_train, y_train)\n",
        "y_pred3 = bnb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred3))\n",
        "print(confusion_matrix(y_test, y_pred3))\n",
        "print(precision_score(y_test, y_pred3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S5O70rceK3pQ",
      "metadata": {
        "id": "S5O70rceK3pQ"
      },
      "outputs": [],
      "source": [
        "#as data is IMBALANCED, precision score matters more than accuracy.\n",
        "#using TfidfVectorizer method, we get precision score = 1 for MultinomialNB\n",
        "#so we will use this only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gq-hSXHwLjkn",
      "metadata": {
        "id": "gq-hSXHwLjkn"
      },
      "outputs": [],
      "source": [
        "#trying out different CLASSIFIER model for the BEST predictions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HspGQxShLn_B",
      "metadata": {
        "id": "HspGQxShLn_B"
      },
      "outputs": [],
      "source": [
        "#creating objects of the classifier models\n",
        "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
        "knc = KNeighborsClassifier()\n",
        "mnb = MultinomialNB()\n",
        "dtc = DecisionTreeClassifier(max_depth=5)\n",
        "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
        "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
        "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
        "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
        "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VJbaA2c8LqQV",
      "metadata": {
        "id": "VJbaA2c8LqQV"
      },
      "outputs": [],
      "source": [
        "#creating a dictionary that maps short names to the corresponding classification models.\n",
        "clfs = {\n",
        "    'SVC' : svc,\n",
        "    'KN' : knc,\n",
        "    'NB': mnb,\n",
        "    'DT': dtc,\n",
        "    'LR': lrc,\n",
        "    'RF': rfc,\n",
        "    'AdaBoost': abc,\n",
        "    'BgC': bc,\n",
        "    'ETC': etc,\n",
        "    'GBDT':gbdt,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yt0KfuNYL7dq",
      "metadata": {
        "id": "yt0KfuNYL7dq"
      },
      "outputs": [],
      "source": [
        "#creating a function which uses train test split data and performing on model and returning the scores\n",
        "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
        "    clf.fit(X_train,y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test,y_pred)\n",
        "    precision = precision_score(y_test,y_pred)\n",
        "\n",
        "    return accuracy,precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pL97j-a4MJ2o",
      "metadata": {
        "id": "pL97j-a4MJ2o"
      },
      "outputs": [],
      "source": [
        "#taking each classifier algorithm, training and testing data, storing the score values and then printing for each\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "\n",
        "for name,clf in clfs.items():\n",
        "\n",
        "    #calling the previously defined function\n",
        "    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)\n",
        "\n",
        "    print(\"For \",name)\n",
        "    print(\"Accuracy - \",current_accuracy)\n",
        "    print(\"Precision - \",current_precision)\n",
        "\n",
        "    accuracy_scores.append(current_accuracy)\n",
        "    precision_scores.append(current_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iD0iWdnNMaZj",
      "metadata": {
        "id": "iD0iWdnNMaZj"
      },
      "outputs": [],
      "source": [
        "#converting the accuracy and precision score values to a dataframe\n",
        "#sorting on the basis of precision value\n",
        "performance = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WV78j7NgMqid",
      "metadata": {
        "id": "WV78j7NgMqid"
      },
      "outputs": [],
      "source": [
        "#precision is 1, we need to maximize the accuracy score.\n",
        "#try using the Voting classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f7456c",
      "metadata": {
        "id": "84f7456c"
      },
      "outputs": [],
      "source": [
        "#Voting classifier of NB, RF and ETC\n",
        "\n",
        "#creating the objects for the classifier classes\n",
        "mnb = MultinomialNB()\n",
        "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
        "rfc = RandomForestClassifier(n_estimators=50, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9192cf",
      "metadata": {
        "id": "ea9192cf"
      },
      "outputs": [],
      "source": [
        "#creating voting object\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "voting = VotingClassifier(estimators=[('rf', rfc), ('nb', mnb), ('et', etc)],voting='soft')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba480c35",
      "metadata": {
        "scrolled": true,
        "id": "ba480c35"
      },
      "outputs": [],
      "source": [
        "#training the data\n",
        "voting.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cded7e12",
      "metadata": {
        "id": "cded7e12"
      },
      "outputs": [],
      "source": [
        "y_pred = voting.predict(X_test)\n",
        "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
        "print(\"Precision\",precision_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e555ef92",
      "metadata": {
        "id": "e555ef92"
      },
      "outputs": [],
      "source": [
        "#precision is still 1 but accuracy dropped down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e02d431",
      "metadata": {
        "id": "5e02d431"
      },
      "outputs": [],
      "source": [
        "#let's try STACKING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2dea307",
      "metadata": {
        "id": "a2dea307"
      },
      "outputs": [],
      "source": [
        "#Stacking of NB, RF and ETC\n",
        "estimators=[('rf', rfc), ('nb', mnb), ('et', etc)]\n",
        "\n",
        "#most weightage to ETC\n",
        "final_estimator=ExtraTreesClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9c5b65",
      "metadata": {
        "id": "ed9c5b65"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf82c7e7",
      "metadata": {
        "id": "cf82c7e7"
      },
      "outputs": [],
      "source": [
        "#training the dataset\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#printing the scores\n",
        "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
        "print(\"Precision\",precision_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd87901b",
      "metadata": {
        "id": "bd87901b"
      },
      "outputs": [],
      "source": [
        "#precision dropped so can't consider it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c897ef2c",
      "metadata": {
        "id": "c897ef2c"
      },
      "outputs": [],
      "source": [
        "#finally, ETC is giving the best results, so we will be using the ETC classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74aa58ec",
      "metadata": {
        "id": "74aa58ec"
      },
      "outputs": [],
      "source": [
        "#model is PREPARED.\n",
        "#now we have to host the website, for that pipeling needs to be done\n",
        "#the text which we will get, has to be transformed first, then vectorized and then apply the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dd2f73",
      "metadata": {
        "id": "75dd2f73"
      },
      "outputs": [],
      "source": [
        "#we will pickle 2 files\n",
        "import pickle\n",
        "pickle.dump(tf,open('vectorizer.pkl','wb'))\n",
        "pickle.dump(etc,open('model.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba83f737",
      "metadata": {
        "id": "ba83f737"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}